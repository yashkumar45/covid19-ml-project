# -*- coding: utf-8 -*-
"""covid19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/138tdnmzXe6-l7ftWMHV0O6yjINA1-j3r
"""

# Commented out IPython magic to ensure Python compatibility.
#importing the important libraries
import numpy as np
import pandas as pd
import  matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import random
import math
import time
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error ,mean_absolute_error
import datetime
import operator
plt.style.use('seaborn')
# %matplotlib inline

from google.colab import files
uploaded = files.upload()
confirmed_cases = pd.read_csv('data.csv')
confirmed_cases.head(477)

from google.colab import files
uploaded = files.upload()
deaths_reported = pd.read_csv('death.csv')
deaths_reported.head()

from google.colab import files
uploaded = files.upload()
recovered_cases = pd.read_csv('recovered.csv')
recovered_cases.head()

#diplay the headset of the dataset
confirmed_cases.head()

deaths_reported.head()

recovered_cases.head()

#extracting all the columns using the .keys  function
cols =confirmed_cases.keys()
cols



#extracting only the dates columns that have the information of confirmed,deaths  and recovered cases
confirmed = confirmed_cases.loc[:, cols[4]:cols[-1]]
# we need all the  parameters from the 4th column to the -1 column

deaths = deaths_reported.loc[:, cols[4]:cols[-1]]

recoveries = recovered_cases.loc[:, cols[4]:cols[-1]]

# check the head of the outbreak cases
confirmed.head()

#finding the total confirmed cases,death caess and the recoverd cases and append them on the 4th empty lists
#Also,calculate the toatal mortality rate which is death_sum/confirmed_cases
dates = confirmed.keys()
world_cases = []
total_deaths = []
mortality_rate = []
total_recovered = []

for  i in dates:
  confirmed_sum= confirmed[i].sum()
  death_sum =deaths[i].sum()
  recovered_sum =recoveries[i].sum()
  world_cases.append(confirmed_sum)
  total_deaths.append(death_sum)
  mortality_rate.append(death_sum/confirmed_sum)
  total_recovered.append(recovered_sum)

#lets display the each of the new created varaiable
confirmed_sum

death_sum

recovered_sum

world_cases

#convert all the dates and the cases in the formm of the numpy arrays
days_since_1_22 = np.array([i for i in range(len(dates))]).reshape(-1,1)
world_cases = np.array(world_cases).reshape(-1,1)
total_deaths = np.array(total_deaths).reshape(-1,1)
total_recoverd = np.array(total_recovered).reshape(-1,1)

days_since_1_22

world_cases

total_deaths

total_recoverd

# future forecasting for the next  10  days
days_in_future = 10
future_forecast = np.array([i for i in range(len(dates)+days_in_future)]).reshape(-1,1)
adjusted_dates = future_forecast[:-10]

future_forecast

#convert  all the integers  into datetime for visualisation
start = '1/12/2020'
start_date = datetime.datetime.strptime(start,'%m/%d/%Y')
future_forecast_dates = []
for i in range(len(future_forecast)):
  future_forecast_dates.append((start_date+datetime.timedelta(days=i)).strftime('%m/%d/%Y'))

#for visualisation with the latest date of 15th of march
latest_confirmed = confirmed_cases[dates[-1]]
latest_deaths = deaths_reported[dates[-1]]
latest_recoveries = recovered_cases[dates[-1]]

latest_confirmed

latest_deaths

latest_recoveries

#finding the list of unique countries
unique_countries = list(confirmed_cases['Country/Region'].unique())
unique_countries

#the next  line of code will basically calculate the total number pf confirmed cases by each country
country_confirmed_cases = []
no_cases = []
for  i  in unique_countries:
  cases = latest_confirmed[confirmed_cases['Country/Region']==i].sum()
  if cases > 0:
    country_confirmed_cases.append(cases)
  else:
      no_cases.append(i)

for i in no_cases:
  unique_countries.remove(i)
unique_countries = [k for k,v in sorted(zip(unique_countries,country_confirmed_cases)]
for i in range(len(unique_countries)):
  country_confirmed_cases[i] = latest_confirmed[confirmed_cases['Country/Region']==unique_countries[i]].sum()

#number of cases per country/region
#finding the list of unique countries
unique_countries = list(confirmed_cases['Country/Region'].unique())
print('Confirmed cases by countries/regions:')
for i in range(len(unique_countries)):
  print(f'{unique_countries[i]}:{confirmed_cases[i]}cases')

#visulise the count
import seaborn as sns
sns.countplot(confirmed_cases['	Country/Region'],label='count')



#visualise the corerelation
import seaborn as sns
plt.figure(figsize=(10,10))
sns.heatmap(confirmed_cases.iloc[:,1:12].corr(),annot=True, fmt='.0%')

#visualise the corerelation
import seaborn as sns
plt.figure(figsize=(10,10))
sns.heatmap(deaths_reported.iloc[:,1:12].corr(),annot=True, fmt='.0%')



#visualise the corerelation
import seaborn as sns
plt.figure(figsize=(10,10))
sns.heatmap(recovered_cases.iloc[:,1:12].corr(),annot=True, fmt='.0%')



# create a pair plot
sns.pairplot(confirmed_cases.iloc[:,1:6])



# create a pair plot
sns.pairplot(deaths_reported.iloc[:,1:6])



#get the corelation of the coluns
confirmed_cases.iloc[:,1:12].corr()

# split the dataset intpo independent and dependent(Y) data sets
X = confirmed_cases.iloc[:,2:31].values
Y = confirmed_cases.iloc[:,1].values



# Split the datset into 75 percent raining and 25 percent testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.25, random_state = 0)



# scale the data(feature scaling)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test= sc.fit_transform(X_test)
X_train



# create a function for the model
def models(X_train,Y_train):
  #Logistic regression
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train,Y_train)

  #Decisssion tree
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion ='entropy', random_state=0)
  tree.fit(X_train,Y_train)

  #Random forest classifire
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10,criterion ='entropy',random_state=0)
  forest.fit(X_train, Y_train)

  #print the model accuracy on the training data
  print('[0]Logistic Regression Training Accuracy:',log.score(X_train,Y_train))
  print('[1]Decision tree classifier training Accuracy:',tree.score(X_train,Y_train))
  print('[2]Random forest classifier Training Accuracy:',forest.score(X_train,Y_train))

  return log,tree,forest



#getting all of the models
model = models(X_train, Y_train)



#test model accuracy on test dat on confusion matrix
from sklearn.metrics import confusion_matrix

for i in range( len(model) ):
  print('Model',i)
  cm = confusion_matrix(Y_test,model[1].predict(X_test))

  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[1][0]
  FP =  cm[0][1]
  print(cm)
  print('Testing Accuracy',(TP + TN)/(TP + TN + FN + FP))



# show another way to get the atrics
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range( len(model) ):
  print('Model',i)
  print( classification_report(Y_test,model[i].predict(X_test)))
  print( accuracy_score(Y_test,model[i].predict(X_test)))



# print the prediction of Random forest classifiesr
pred = model[2].predict(X_test)
print(pred)
print()
print(Y_test)